{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.6.1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import openai\n",
    "openai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:167: UserWarning: As of openai>=1.0.0, Azure endpoints should be specified via the `azure_endpoint` param not `openai_api_base` (or alias `base_url`). Updating `openai_api_base` from https://seorevolution-gpt4.openai.azure.com/ to https://seorevolution-gpt4.openai.azure.com/openai.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:174: UserWarning: As of openai>=1.0.0, if `deployment_name` (or alias `azure_deployment`) is specified then `openai_api_base` (or alias `base_url`) should not be. Instead use `deployment_name` (or alias `azure_deployment`) and `azure_endpoint`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\azure_openai.py:182: UserWarning: As of openai>=1.0.0, if `openai_api_base` (or alias `base_url`) is specified it is expected to be of the form https://example-resource.azure.openai.com/openai/deployments/example-deployment. Updating https://seorevolution-gpt4.openai.azure.com/ to https://seorevolution-gpt4.openai.azure.com/openai.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_type=\"azure\",\n",
    "    openai_api_key= os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    openai_api_base=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    model=os.getenv(\"AZURE_OPENAI_MODEL\"),\n",
    "    openai_api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hi AI, how are you today?\"),\n",
    "    AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
    "    HumanMessage(content=\"I'd like to understand string theory.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory aims to reconcile quantum mechanics, which describes the behavior of tiny particles, with general relativity, which describes the behavior of large-scale objects like planets and galaxies.\\n\\nThe theory is based on the idea that strings vibrate at different frequencies, and these vibrations give rise to the particles' distinctive properties, such as mass and charge. Each string's vibrational state corresponds to a different fundamental particle. \\n\\nThere are different versions of string theory, including Type I, Type IIA, Type IIB, and two flavors of heterotic string theory. The different theories allow for the existence of different types of strings (open and closed strings) and have different types of extra-dimensional structures. \\n\\nString theory also predicts the existence of more than the three spatial dimensions that we experience in our everyday life. These extra dimensions could be compactified, or hidden from our perception.\\n\\nIt's important to note that string theory is still a theoretical construct and has yet to be confirmed through experimental evidence. Its mathematical beauty and potential to unify the fundamental forces of nature, however, have made it a popular area of research among theoretical physicists.\", response_metadata={'token_usage': {'completion_tokens': 247, 'prompt_tokens': 53, 'total_tokens': 300}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a10d4e30-e942-4200-8e88-17ab71e657b5-0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = llm(messages)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory aims to reconcile quantum mechanics, which describes the behavior of tiny particles, with general relativity, which describes the behavior of large-scale objects like planets and galaxies.\n",
      "\n",
      "The theory is based on the idea that strings vibrate at different frequencies, and these vibrations give rise to the particles' distinctive properties, such as mass and charge. Each string's vibrational state corresponds to a different fundamental particle. \n",
      "\n",
      "There are different versions of string theory, including Type I, Type IIA, Type IIB, and two flavors of heterotic string theory. The different theories allow for the existence of different types of strings (open and closed strings) and have different types of extra-dimensional structures. \n",
      "\n",
      "String theory also predicts the existence of more than the three spatial dimensions that we experience in our everyday life. These extra dimensions could be compactified, or hidden from our perception.\n",
      "\n",
      "It's important to note that string theory is still a theoretical construct and has yet to be confirmed through experimental evidence. Its mathematical beauty and potential to unify the fundamental forces of nature, however, have made it a popular area of research among theoretical physicists.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='Hi AI, how are you today?'),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
       " HumanMessage(content=\"I'd like to understand string theory.\"),\n",
       " AIMessage(content=\"String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory aims to reconcile quantum mechanics, which describes the behavior of tiny particles, with general relativity, which describes the behavior of large-scale objects like planets and galaxies.\\n\\nThe theory is based on the idea that strings vibrate at different frequencies, and these vibrations give rise to the particles' distinctive properties, such as mass and charge. Each string's vibrational state corresponds to a different fundamental particle. \\n\\nThere are different versions of string theory, including Type I, Type IIA, Type IIB, and two flavors of heterotic string theory. The different theories allow for the existence of different types of strings (open and closed strings) and have different types of extra-dimensional structures. \\n\\nString theory also predicts the existence of more than the three spatial dimensions that we experience in our everyday life. These extra dimensions could be compactified, or hidden from our perception.\\n\\nIt's important to note that string theory is still a theoretical construct and has yet to be confirmed through experimental evidence. Its mathematical beauty and potential to unify the fundamental forces of nature, however, have made it a popular area of research among theoretical physicists.\", response_metadata={'token_usage': {'completion_tokens': 247, 'prompt_tokens': 53, 'total_tokens': 300}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a10d4e30-e942-4200-8e88-17ab71e657b5-0')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physicists believe that string theory can produce a \"unified theory\" because it has the potential to reconcile the two major pillars of modern physics: quantum mechanics and general relativity.\n",
      "\n",
      "Quantum mechanics excellently describes the interactions of the smallest particles, like electrons and quarks, while general relativity provides a framework for understanding the universe at its largest scales, like stars, galaxies, and the universe itself. However, the two theories fundamentally conflict with each other in certain extreme situations, like in the heart of black holes or at the moment of the Big Bang.\n",
      "\n",
      "String theory, however, provides a framework in which these two theories can be reconciled. In string theory, particles are not point-like, but are instead one-dimensional strings. This eliminates the problem of point-like singularities in general relativity since strings, being extended objects, do not allow for such singularities. \n",
      "\n",
      "Moreover, string theory naturally includes gravity, something no other quantum theory has been able to achieve. In string theory, the graviton (the hypothetical quantum of gravity) is included as one possible vibrational mode of the strings.\n",
      "\n",
      "Finally, string theory has the potential to unify all four fundamental forces of nature - gravity, electromagnetism, strong nuclear force, and weak nuclear force - into a single framework, something that has been a long-standing goal of theoretical physics. \n",
      "\n",
      "That said, it's important to note that, while promising and mathematically elegant, string theory is still in the realm of the theoretical and has yet to be confirmed by experimental evidence.\n"
     ]
    }
   ],
   "source": [
    "# now create a new user prompt\n",
    "prompt = HumanMessage(\n",
    "    content=\"Why do physicists believe it can produce a 'unified theory'?\"\n",
    ")\n",
    "# add to messages\n",
    "messages.append(prompt)\n",
    "\n",
    "# send to chat-gpt\n",
    "res = llm(messages)\n",
    "\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with Hallucination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='Hi AI, how are you today?'),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
       " HumanMessage(content=\"I'd like to understand string theory.\"),\n",
       " AIMessage(content=\"String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory aims to reconcile quantum mechanics, which describes the behavior of tiny particles, with general relativity, which describes the behavior of large-scale objects like planets and galaxies.\\n\\nThe theory is based on the idea that strings vibrate at different frequencies, and these vibrations give rise to the particles' distinctive properties, such as mass and charge. Each string's vibrational state corresponds to a different fundamental particle. \\n\\nThere are different versions of string theory, including Type I, Type IIA, Type IIB, and two flavors of heterotic string theory. The different theories allow for the existence of different types of strings (open and closed strings) and have different types of extra-dimensional structures. \\n\\nString theory also predicts the existence of more than the three spatial dimensions that we experience in our everyday life. These extra dimensions could be compactified, or hidden from our perception.\\n\\nIt's important to note that string theory is still a theoretical construct and has yet to be confirmed through experimental evidence. Its mathematical beauty and potential to unify the fundamental forces of nature, however, have made it a popular area of research among theoretical physicists.\", response_metadata={'token_usage': {'completion_tokens': 247, 'prompt_tokens': 53, 'total_tokens': 300}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a10d4e30-e942-4200-8e88-17ab71e657b5-0'),\n",
       " HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\")]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.'),\n",
       " HumanMessage(content='Hi AI, how are you today?'),\n",
       " AIMessage(content=\"I'm great thank you. How can I help you?\"),\n",
       " HumanMessage(content=\"I'd like to understand string theory.\"),\n",
       " AIMessage(content=\"String theory is a theoretical framework in which the point-like particles of particle physics are replaced by one-dimensional objects called strings. String theory aims to reconcile quantum mechanics, which describes the behavior of tiny particles, with general relativity, which describes the behavior of large-scale objects like planets and galaxies.\\n\\nThe theory is based on the idea that strings vibrate at different frequencies, and these vibrations give rise to the particles' distinctive properties, such as mass and charge. Each string's vibrational state corresponds to a different fundamental particle. \\n\\nThere are different versions of string theory, including Type I, Type IIA, Type IIB, and two flavors of heterotic string theory. The different theories allow for the existence of different types of strings (open and closed strings) and have different types of extra-dimensional structures. \\n\\nString theory also predicts the existence of more than the three spatial dimensions that we experience in our everyday life. These extra dimensions could be compactified, or hidden from our perception.\\n\\nIt's important to note that string theory is still a theoretical construct and has yet to be confirmed through experimental evidence. Its mathematical beauty and potential to unify the fundamental forces of nature, however, have made it a popular area of research among theoretical physicists.\", response_metadata={'token_usage': {'completion_tokens': 247, 'prompt_tokens': 53, 'total_tokens': 300}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-a10d4e30-e942-4200-8e88-17ab71e657b5-0'),\n",
       " HumanMessage(content=\"Why do physicists believe it can produce a 'unified theory'?\"),\n",
       " AIMessage(content='Physicists believe that string theory can produce a \"unified theory\" because it has the potential to reconcile the two major pillars of modern physics: quantum mechanics and general relativity.\\n\\nQuantum mechanics excellently describes the interactions of the smallest particles, like electrons and quarks, while general relativity provides a framework for understanding the universe at its largest scales, like stars, galaxies, and the universe itself. However, the two theories fundamentally conflict with each other in certain extreme situations, like in the heart of black holes or at the moment of the Big Bang.\\n\\nString theory, however, provides a framework in which these two theories can be reconciled. In string theory, particles are not point-like, but are instead one-dimensional strings. This eliminates the problem of point-like singularities in general relativity since strings, being extended objects, do not allow for such singularities. \\n\\nMoreover, string theory naturally includes gravity, something no other quantum theory has been able to achieve. In string theory, the graviton (the hypothetical quantum of gravity) is included as one possible vibrational mode of the strings.\\n\\nFinally, string theory has the potential to unify all four fundamental forces of nature - gravity, electromagnetism, strong nuclear force, and weak nuclear force - into a single framework, something that has been a long-standing goal of theoretical physics. \\n\\nThat said, it\\'s important to note that, while promising and mathematically elegant, string theory is still in the realm of the theoretical and has yet to be confirmed by experimental evidence.', response_metadata={'token_usage': {'completion_tokens': 309, 'prompt_tokens': 321, 'total_tokens': 630}, 'model_name': 'gpt-4-32k', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-05978f44-fc3e-4672-8648-ede4d6c7a95c-0')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new user prompt\n",
    "prompt=HumanMessage(content=\"What is so special about Llama2?\")\n",
    "\n",
    "# Adding above prompt into messages\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send above messages into openai\n",
    "res=llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Llama2\" could refer to a few different things depending on the context, so I'll provide a broad answer. \n",
      "\n",
      "In the context of biology and genetics, LAMA2 is a gene that provides instructions for making a protein that is essential for normal muscle function. Mutations in this gene can cause a form of congenital muscular dystrophy.\n",
      "\n",
      "If you're referring to a software or programming library, there is a Llama library for Python programming language used for data analysis tasks. \n",
      "\n",
      "If you're referring to something else, could you please provide more context so I could give you a more precise answer?\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' as our chatbot does not contain information regarding Llama. It is clear that\\nLLM doesnot know about this.\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' as our chatbot does not contain information regarding Llama. It is clear that\n",
    "LLM doesnot know about this.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding latest answer to chatbot\n",
    "messages.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Create a new user prompt\n",
    "prompt=HumanMessage(content=\"Can you tell me about LLM Chain in Langchain?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding prompt to bot\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but as of my current knowledge database update in October 2021, there's no specific information available on an \"LLM Chain in Langchain\". It might be a new development that has happened after my last update, or there may be a mix-up in the terms. \n",
      "\n",
      "Langchain is a platform that is said to use artificial intelligence technology for language translation. However, specific details about an \"LLM Chain\" within Langchain aren't readily available.\n",
      "\n",
      "I'd recommend checking the most recent and reliable sources, or the official Langchain documentation, for the most accurate and up-to-date information. Please note that the world of technology and blockchain is ever-evolving, and new updates or developments might have occurred after my last programming update.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Openai does not know about this LLama because it has information till Oct. 2021 and It  got released later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another way of feeding knowledge into LLMs. It is called source knowledge and it refers to any information fed into the LLM via the prompt. We can try that with the LLMChain question. We can take a description of this object from the LangChain documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_knowledge='\\n'.join(llmchain_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeding additional knowledge into our prompts\n",
    "query=\"Can you tell me about LLM Chain in LAngchain?\"\n",
    "augmented_prompt=f''' USing the context below answer the query.\n",
    "Context: \n",
    "{source_knowledge}\n",
    "\n",
    "Query: {query}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Sending above augmented prompt to bot\n",
    "prompt=HumanMessage(content=augmented_prompt)\n",
    "messages.append(prompt)\n",
    "res=llm(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of LangChain, an LLMChain is a common type of chain that includes a PromptTemplate, a model (LLM or ChatModel), and an optional output parser. \n",
      "\n",
      "The LLMChain works by taking multiple input variables and using the PromptTemplate to format them into a prompt, which is then passed to the model. The model generates the output based on this formatted prompt. If an OutputParser is provided, it is used to parse the model's output into a final, typically more user-friendly format.\n",
      "\n",
      "The concept of \"chains\" in LangChain is quite generic, referring to a sequence of modular components (or other chains) combined in a certain way to accomplish a specific task or use case. \n",
      "\n",
      "In LangChain framework, the chains, such as LLMChain, are used to develop applications powered by language models. The framework aims to create applications that are not only able to use language models via an API, but also connect these models to other data sources and allow them to interact with their environment.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier it was not able to answer the same query. Now It is able to answer after provoding some context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "Now We will import a  on Hugging Face Dataset Libray. This dataset contains collection of research paper which will serve as external knowledge base for the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets==2.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#dataset = load_dataset(\n",
    "#   \"jamescalam/llama-2-arxiv-papers-chunked\",\n",
    "#    split=\"train\"\n",
    "#)\n",
    "\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
      "    num_rows: 4838\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Specify the path to the specific dataset file within the cached directory\n",
    "dataset_path = \"C:/Users/lenovo/.cache/huggingface/datasets/jamescalam___json/jamescalam--llama-2-arxiv-papers-chunked-ea255a807f3039a6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/json-train.arrow\"\n",
    "\n",
    "# Load the dataset from the specified file\n",
    "dataset = Dataset.from_file(dataset_path)\n",
    "\n",
    "# Print some information about the dataset\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1102.0183',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'High-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nTechnical Report No. IDSIA-01-11\\nJanuary 2011\\nIDSIA / USI-SUPSI\\nDalle Molle Institute for Arti\\x0ccial Intelligence\\nGalleria 2, 6928 Manno, Switzerland\\nIDSIA is a joint institute of both University of Lugano (USI) and University of Applied Sciences of Southern Switzerland (SUPSI),\\nand was founded in 1988 by the Dalle Molle Foundation which promoted quality of life.\\nThis work was partially supported by the Swiss Commission for Technology and Innovation (CTI), Project n. 9688.1 IFF:\\nIntelligent Fill in Form.arXiv:1102.0183v1  [cs.AI]  1 Feb 2011\\nTechnical Report No. IDSIA-01-11 1\\nHigh-Performance Neural Networks\\nfor Visual Object Classi\\x0ccation\\nDan C. Cire\\x18 san, Ueli Meier, Jonathan Masci,\\nLuca M. Gambardella and J\\x7f urgen Schmidhuber\\nJanuary 2011\\nAbstract\\nWe present a fast, fully parameterizable GPU implementation of Convolutional Neural\\nNetwork variants. Our feature extractors are neither carefully designed nor pre-wired, but',\n",
       " 'id': '1102.0183',\n",
       " 'title': 'High-Performance Neural Networks for Visual Object Classification',\n",
       " 'summary': 'We present a fast, fully parameterizable GPU implementation of Convolutional\\nNeural Network variants. Our feature extractors are neither carefully designed\\nnor pre-wired, but rather learned in a supervised way. Our deep hierarchical\\narchitectures achieve the best published results on benchmarks for object\\nclassification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\\nerror rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\\nback-propagation perform better than more shallow ones. Learning is\\nsurprisingly rapid. NORB is completely trained within five epochs. Test error\\nrates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\\nrespectively.',\n",
       " 'source': 'http://arxiv.org/pdf/1102.0183',\n",
       " 'authors': ['Dan C. Cireşan',\n",
       "  'Ueli Meier',\n",
       "  'Jonathan Masci',\n",
       "  'Luca M. Gambardella',\n",
       "  'Jürgen Schmidhuber'],\n",
       " 'categories': ['cs.AI', 'cs.NE'],\n",
       " 'comment': '12 pages, 2 figures, 5 tables',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.AI',\n",
       " 'published': '20110201',\n",
       " 'updated': '20110201',\n",
       " 'references': []}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We present a fast, fully parameterizable GPU implementation of Convolutional\n",
      "Neural Network variants. Our feature extractors are neither carefully designed\n",
      "nor pre-wired, but rather learned in a supervised way. Our deep hierarchical\n",
      "architectures achieve the best published results on benchmarks for object\n",
      "classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with\n",
      "error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple\n",
      "back-propagation perform better than more shallow ones. Learning is\n",
      "surprisingly rapid. NORB is completely trained within five epochs. Test error\n",
      "rates on MNIST drop to 2.42%, 0.97% and 0.48% after 1, 3 and 17 epochs,\n",
      "respectively.\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABove dataset is collection of academic paper from ArXiv, a repo of electronic preprints approved for publication after moderation.\n",
    "AS LLM can answer only those question which was present during training. Later question it cant answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the knowledge base\n",
    "We have the dataset that can serve as external knowledge base. Now our task is to convert it into embedding model and vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pinecone vectorDB\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\")\n",
    "index = pc.Index(\"Index name in pinecone\") #index of vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats() #checking the status of index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our index is empty.  Its a vector index so it needs vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import AzureOpenAIEmbeddings  \n",
    "import os  \n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "  \n",
    "embed_model = AzureOpenAIEmbeddings(  \n",
    "    api_key=os.getenv(\"AZURE_OPENAI_EMBEDDING_KEY\"),  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_EMBEDDING_ENDPOINT\"),  \n",
    "    api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),  # Use the correct API version  \n",
    "    model=\"text-embedding-ada-002\",\n",
    "    azure_deployment=\"Embeddings\"\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res),len(res[0]) #there are total two  chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us Embed and index whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm #for progress bar\n",
    "data=dataset.to_pandas() #converting into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doi</th>\n",
       "      <th>chunk-id</th>\n",
       "      <th>chunk</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>comment</th>\n",
       "      <th>journal_ref</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>published</th>\n",
       "      <th>updated</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>0</td>\n",
       "      <td>High-Performance Neural Networks\\nfor Visual O...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>1</td>\n",
       "      <td>January 2011\\nAbstract\\nWe present a fast, ful...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>2</td>\n",
       "      <td>promising architectures for such tasks. The mo...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>3</td>\n",
       "      <td>Mutch and Lowe, 2008), whose \flters are \fxed, ...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1102.0183</td>\n",
       "      <td>4</td>\n",
       "      <td>We evaluate various networks on the handwritte...</td>\n",
       "      <td>1102.0183</td>\n",
       "      <td>High-Performance Neural Networks for Visual Ob...</td>\n",
       "      <td>We present a fast, fully parameterizable GPU i...</td>\n",
       "      <td>http://arxiv.org/pdf/1102.0183</td>\n",
       "      <td>[Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...</td>\n",
       "      <td>[cs.AI, cs.NE]</td>\n",
       "      <td>12 pages, 2 figures, 5 tables</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>20110201</td>\n",
       "      <td>20110201</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>2307.09288</td>\n",
       "      <td>315</td>\n",
       "      <td>BytheCentralLimitTheorem, Zntendstowardsastand...</td>\n",
       "      <td>2307.09288</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>In this work, we develop and release Llama 2, ...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "      <td>[Hugo Touvron, Louis Martin, Kevin Stone, Pete...</td>\n",
       "      <td>[cs.CL, cs.AI]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20230718</td>\n",
       "      <td>20230719</td>\n",
       "      <td>[{'id': '2305.13245', 'title': 'GQA: Training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>2307.09288</td>\n",
       "      <td>316</td>\n",
       "      <td>Table 52 presents a model card (Mitchell et al...</td>\n",
       "      <td>2307.09288</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>In this work, we develop and release Llama 2, ...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "      <td>[Hugo Touvron, Louis Martin, Kevin Stone, Pete...</td>\n",
       "      <td>[cs.CL, cs.AI]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20230718</td>\n",
       "      <td>20230719</td>\n",
       "      <td>[{'id': '2305.13245', 'title': 'GQA: Training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4835</th>\n",
       "      <td>2307.09288</td>\n",
       "      <td>317</td>\n",
       "      <td>models will be released as we improve model sa...</td>\n",
       "      <td>2307.09288</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>In this work, we develop and release Llama 2, ...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "      <td>[Hugo Touvron, Louis Martin, Kevin Stone, Pete...</td>\n",
       "      <td>[cs.CL, cs.AI]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20230718</td>\n",
       "      <td>20230719</td>\n",
       "      <td>[{'id': '2305.13245', 'title': 'GQA: Training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>2307.09288</td>\n",
       "      <td>318</td>\n",
       "      <td>Training Factors We usedcustomtraininglibrarie...</td>\n",
       "      <td>2307.09288</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>In this work, we develop and release Llama 2, ...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "      <td>[Hugo Touvron, Louis Martin, Kevin Stone, Pete...</td>\n",
       "      <td>[cs.CL, cs.AI]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20230718</td>\n",
       "      <td>20230719</td>\n",
       "      <td>[{'id': '2305.13245', 'title': 'GQA: Training ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4837</th>\n",
       "      <td>2307.09288</td>\n",
       "      <td>319</td>\n",
       "      <td>Evaluation Results\\nSee evaluations for pretra...</td>\n",
       "      <td>2307.09288</td>\n",
       "      <td>Llama 2: Open Foundation and Fine-Tuned Chat M...</td>\n",
       "      <td>In this work, we develop and release Llama 2, ...</td>\n",
       "      <td>http://arxiv.org/pdf/2307.09288</td>\n",
       "      <td>[Hugo Touvron, Louis Martin, Kevin Stone, Pete...</td>\n",
       "      <td>[cs.CL, cs.AI]</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>20230718</td>\n",
       "      <td>20230719</td>\n",
       "      <td>[{'id': '2305.13245', 'title': 'GQA: Training ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4838 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             doi chunk-id                                              chunk  \\\n",
       "0      1102.0183        0  High-Performance Neural Networks\\nfor Visual O...   \n",
       "1      1102.0183        1  January 2011\\nAbstract\\nWe present a fast, ful...   \n",
       "2      1102.0183        2  promising architectures for such tasks. The mo...   \n",
       "3      1102.0183        3  Mutch and Lowe, 2008), whose \n",
       "lters are \n",
       "xed, ...   \n",
       "4      1102.0183        4  We evaluate various networks on the handwritte...   \n",
       "...          ...      ...                                                ...   \n",
       "4833  2307.09288      315  BytheCentralLimitTheorem, Zntendstowardsastand...   \n",
       "4834  2307.09288      316  Table 52 presents a model card (Mitchell et al...   \n",
       "4835  2307.09288      317  models will be released as we improve model sa...   \n",
       "4836  2307.09288      318  Training Factors We usedcustomtraininglibrarie...   \n",
       "4837  2307.09288      319  Evaluation Results\\nSee evaluations for pretra...   \n",
       "\n",
       "              id                                              title  \\\n",
       "0      1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "1      1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "2      1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "3      1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "4      1102.0183  High-Performance Neural Networks for Visual Ob...   \n",
       "...          ...                                                ...   \n",
       "4833  2307.09288  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "4834  2307.09288  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "4835  2307.09288  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "4836  2307.09288  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "4837  2307.09288  Llama 2: Open Foundation and Fine-Tuned Chat M...   \n",
       "\n",
       "                                                summary  \\\n",
       "0     We present a fast, fully parameterizable GPU i...   \n",
       "1     We present a fast, fully parameterizable GPU i...   \n",
       "2     We present a fast, fully parameterizable GPU i...   \n",
       "3     We present a fast, fully parameterizable GPU i...   \n",
       "4     We present a fast, fully parameterizable GPU i...   \n",
       "...                                                 ...   \n",
       "4833  In this work, we develop and release Llama 2, ...   \n",
       "4834  In this work, we develop and release Llama 2, ...   \n",
       "4835  In this work, we develop and release Llama 2, ...   \n",
       "4836  In this work, we develop and release Llama 2, ...   \n",
       "4837  In this work, we develop and release Llama 2, ...   \n",
       "\n",
       "                               source  \\\n",
       "0      http://arxiv.org/pdf/1102.0183   \n",
       "1      http://arxiv.org/pdf/1102.0183   \n",
       "2      http://arxiv.org/pdf/1102.0183   \n",
       "3      http://arxiv.org/pdf/1102.0183   \n",
       "4      http://arxiv.org/pdf/1102.0183   \n",
       "...                               ...   \n",
       "4833  http://arxiv.org/pdf/2307.09288   \n",
       "4834  http://arxiv.org/pdf/2307.09288   \n",
       "4835  http://arxiv.org/pdf/2307.09288   \n",
       "4836  http://arxiv.org/pdf/2307.09288   \n",
       "4837  http://arxiv.org/pdf/2307.09288   \n",
       "\n",
       "                                                authors      categories  \\\n",
       "0     [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "1     [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "2     [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "3     [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "4     [Dan C. Cireşan, Ueli Meier, Jonathan Masci, L...  [cs.AI, cs.NE]   \n",
       "...                                                 ...             ...   \n",
       "4833  [Hugo Touvron, Louis Martin, Kevin Stone, Pete...  [cs.CL, cs.AI]   \n",
       "4834  [Hugo Touvron, Louis Martin, Kevin Stone, Pete...  [cs.CL, cs.AI]   \n",
       "4835  [Hugo Touvron, Louis Martin, Kevin Stone, Pete...  [cs.CL, cs.AI]   \n",
       "4836  [Hugo Touvron, Louis Martin, Kevin Stone, Pete...  [cs.CL, cs.AI]   \n",
       "4837  [Hugo Touvron, Louis Martin, Kevin Stone, Pete...  [cs.CL, cs.AI]   \n",
       "\n",
       "                            comment journal_ref primary_category published  \\\n",
       "0     12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "1     12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "2     12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "3     12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "4     12 pages, 2 figures, 5 tables        None            cs.AI  20110201   \n",
       "...                             ...         ...              ...       ...   \n",
       "4833                           None        None            cs.CL  20230718   \n",
       "4834                           None        None            cs.CL  20230718   \n",
       "4835                           None        None            cs.CL  20230718   \n",
       "4836                           None        None            cs.CL  20230718   \n",
       "4837                           None        None            cs.CL  20230718   \n",
       "\n",
       "       updated                                         references  \n",
       "0     20110201                                                 []  \n",
       "1     20110201                                                 []  \n",
       "2     20110201                                                 []  \n",
       "3     20110201                                                 []  \n",
       "4     20110201                                                 []  \n",
       "...        ...                                                ...  \n",
       "4833  20230719  [{'id': '2305.13245', 'title': 'GQA: Training ...  \n",
       "4834  20230719  [{'id': '2305.13245', 'title': 'GQA: Training ...  \n",
       "4835  20230719  [{'id': '2305.13245', 'title': 'GQA: Training ...  \n",
       "4836  20230719  [{'id': '2305.13245', 'title': 'GQA: Training ...  \n",
       "4837  20230719  [{'id': '2305.13245', 'title': 'GQA: Training ...  \n",
       "\n",
       "[4838 rows x 15 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source',\n",
       "       'authors', 'categories', 'comment', 'journal_ref', 'primary_category',\n",
       "       'published', 'updated', 'references'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [05:26<00:00,  6.65s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    # get text to embed\n",
    "    texts = [x['chunk'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['chunk'],\n",
    "         'source': x['source'],\n",
    "         'title': x['title']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.047,\n",
       " 'namespaces': {'': {'vector_count': 4700}},\n",
       " 'total_vector_count': 4700}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector index can be check using describe_index_stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "# we have stored full data in the form of embeddings in a vector database. \n",
    "# lets connect this knowledge base with chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.pinecone.Pinecone` was deprecated in langchain-community 0.0.18 and will be removed in 0.2.0. An updated version of the class exists in the langchain-pinecone package and should be used instead. To use it run `pip install -U langchain-pinecone` and import as `from langchain_pinecone import Pinecone`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\lenovo\\anaconda3\\Lib\\site-packages\\langchain_community\\vectorstores\\pinecone.py:68: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.04838,\n",
       " 'namespaces': {'': {'vector_count': 4838}},\n",
       " 'total_vector_count': 4838}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Using this vectorstore we can already query the index and see if we have any relevant information given our question about Llama 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom\\x03\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\\npreferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\\ncomputeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nL/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'}),\n",
       " Document(page_content='Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\\nGrave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\\narXiv:2302.13971 , 2023.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\\nand Illia Polosukhin. Attention is all you need, 2017.\\nOriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\\nDavid H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\\nmulti-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint', metadata={'source': 'http://arxiv.org/pdf/2307.09288', 'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models'})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is so special about Llama 2?\"\n",
    "\n",
    "vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return a lot of text here and it's not that clear what we need or what is relevant. Fortunately, our LLM will be able to parse this information much faster than us. All we need is to connect the output from our vectorstore to our chat chatbot. To do that we can use the same logic as we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_prompt(query:str):\n",
    "    #getting top 3 results from knowledge base(vectore DB)\n",
    "    results=vectorstore.similarity_search(query, k=3) #k is nearest neighbours\n",
    "    #getting text from results\n",
    "    source_knowledge='\\n'.join(x.page_content for x in results)\n",
    "    #feeding this into augmented prompt\n",
    "    augmented_prompt=f'''\n",
    "    Using the context below,  answer the following question.\n",
    "    Context: \n",
    "    {source_knowledge}\n",
    "    \n",
    "    Query: {query}\n",
    "    \n",
    "    '''\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Using the context below,  answer the following question.\n",
      "    Context: \n",
      "    Alan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\n",
      "Ross Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\n",
      "Angela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\n",
      "Sergey Edunov Thomas Scialom\u0003\n",
      "GenAI, Meta\n",
      "Abstract\n",
      "In this work, we develop and release Llama 2, a collection of pretrained and ﬁne-tuned\n",
      "large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",
      "Our ﬁne-tuned LLMs, called L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , are optimized for dialogue use cases. Our\n",
      "models outperform open-source chat models on most benchmarks we tested, and based on\n",
      "ourhumanevaluationsforhelpfulnessandsafety,maybeasuitablesubstituteforclosedsource models. We provide a detailed description of our approach to ﬁne-tuning and safety\n",
      "asChatGPT,BARD,andClaude. TheseclosedproductLLMsareheavilyﬁne-tunedtoalignwithhuman\n",
      "preferences, which greatly enhances their usability and safety. This step can require signiﬁcant costs in\n",
      "computeandhumanannotation,andisoftennottransparentoreasilyreproducible,limitingprogresswithin\n",
      "the community to advance AI alignment research.\n",
      "In this work, we develop and release Llama 2, a family of pretrained and ﬁne-tuned LLMs, L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle and\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc , at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\n",
      "L/l.sc/a.sc/m.sc/a.sc /two.taboldstyle-C/h.sc/a.sc/t.sc models generally perform better than existing open-source models. They also appear to\n",
      "be on par with some of the closed-source models, at least on the human evaluations we performed (see\n",
      "Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard\n",
      "Grave, and Guillaume Lample. Llama: Open and eﬃcient foundation language models. arXiv preprint\n",
      "arXiv:2302.13971 , 2023.\n",
      "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,\n",
      "and Illia Polosukhin. Attention is all you need, 2017.\n",
      "Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,\n",
      "David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using\n",
      "multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.\n",
      "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and HannanehHajishirzi. Self-instruct: Aligninglanguagemodel withselfgeneratedinstructions. arXivpreprint\n",
      "    \n",
      "    Query: What is so special about Llama 2?\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#use this augmented prompt for getting the results\n",
    "query = \"What is so special about Llama 2?\"\n",
    "print(aug_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LLM Chain in Langchain refers to a sequence of processes involving a Large Language Model (LLM). This chain includes several modular components that accomplish a specific use case in Langchain's framework for developing applications powered by language models.\n",
      "\n",
      "In an LLM Chain, the process typically begins with multiple input variables. These are then formatted into a prompt using a PromptTemplate. This prompt is passed to the LLM for processing. Once the LLM has generated an output based on the prompt, this output can be parsed into a final format using an optional OutputParser.\n",
      "\n",
      "The LLM Chain is highly versatile and can be adjusted or modified to suit different needs or applications. This modular approach allows for a wide range of interactions and output types, making it a powerful tool in the Langchain framework.\n",
      "\n",
      "It's important to note that the LLMs themselves have been pre-trained and fine-tuned for specific uses, such as dialogues. They have been optimized to perform complex reasoning tasks and to interact with humans in an intuitive and engaging manner.\n"
     ]
    }
   ],
   "source": [
    "# pass this context to chat model\n",
    "prompt=HumanMessage(content=aug_prompt(query))\n",
    "\n",
    "# add it to messages\n",
    "messages.append(prompt)\n",
    "res=llm(messages)\n",
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asking more questions\n",
    "prompt=HumanMessage(content=\"What safety measure should we take while using LLama2?\"\n",
    "                    )\n",
    "res=llm(messages+[prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text you provided does not include specific safety measures to take while using Llama2. However, in general, when using any large language model (LLM) such as Llama2, some safety considerations could include:\n",
      "\n",
      "1. Privacy: Ensure that sensitive information is not fed into the model or generated by the model. This includes personally identifiable information (PII) or confidential data.\n",
      "\n",
      "2. Content Control: Monitor the content generated by the model. LLMs have the potential to generate inappropriate or harmful content, so it's important to have safeguards in place.\n",
      "\n",
      "3. Misinformation: Be aware that LLMs do not fact-check the information they generate. They can inadvertently spread misinformation or disinformation.\n",
      "\n",
      "4. Dependence on the Model: LLMs like Llama2 should be used as a tool, not a definitive source of information or action. Users should cross-verify information and not solely rely on the model's output.\n",
      "\n",
      "5. Ethical Use: Ensure the use of the model aligns with ethical guidelines, including not using it to deceive others, spread harmful content, or for any illegal activities.\n",
      "\n",
      "6. Understanding Limitations: LLMs have limitations and can make mistakes. They do not understand context in the same way humans do.\n",
      "\n",
      "For Llama2 specifically, it would be best to consult any safety documentation or guidelines provided by the developers or maintainers of the model.\n"
     ]
    }
   ],
   "source": [
    "print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
